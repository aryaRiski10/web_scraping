# -*- coding: utf-8 -*-
"""Scraping_Glints.ipynb

Automatically generated by Colaboratory.

Original file is located at
    https://colab.research.google.com/drive/1fkGXrgzDCrx38DNK12scDg3IbGTyDJpi

"""
"""Selenium libraries for infinite loop"""
import encodings
from inspect import trace
from socket import timeout
from selenium import webdriver
from selenium.webdriver.common.by import By
from selenium.webdriver.support.ui import WebDriverWait
from selenium.webdriver.support import expected_conditions as EC
from selenium.common.exceptions import TimeoutException
from webdriver_manager.chrome import ChromeDriverManager
from selenium.webdriver.chrome.service import Service

"""------------------------------------------------------"""

"""------------------------------------------------------"""

from tqdm import tqdm
from bs4 import BeautifulSoup
import datetime
from dateutil.relativedelta import relativedelta
from requests.adapters import HTTPAdapter

import csv
import requests
import re
import time
import pandas as pd
import db_mysql
import psutil
import tracemalloc
import os
import memory_profiler

# inner psutil function
def process_memory():
    process = psutil.Process(os.getpid())
    mem_info = process.memory_info()
    return mem_info.rss

# decorator function
def profile(func):
    def wrapper(*args, **kwargs):
        mem_before = process_memory()
        result = func(*args, **kwargs)
        mem_after = process_memory()
        print("{}:consumed memory: {:,}".format(
            func.__name__,
            mem_before, mem_after, mem_after - mem_before))
 
        return result
    return wrapper

@profile
def scrapeData(tableName):
  start = time.time()
  net_stat = psutil.net_io_counters()
  net_in_start = net_stat.bytes_recv
  net_out_start = net_stat.bytes_sent
  
  global temp
  
  keywords = ["front end developer"]
  # keywords = ["accounting and finance","administration and coordination","architecture and engineering",
  #           "arts and sports","customer service","education and training","general services","health and medical",
  #           "hospitality and tourism","human resources","it and software","software engineer","legal","management and consultancy",
  #           "manufacturing and production","media and creatives","public service and ngos","safety and security"
  #           "sales and marketing","sciences","supply chain","writing and content"]

  temp = pd.DataFrame(columns=['title','company','location','requirement','posted','image','link'])

  # Set driver untuk melakukan scrape dengan selenium
  def set_driver():
    PATH = Service("path/chromedriver.exe")
    chrome_options = webdriver.ChromeOptions()
    chrome_options.add_argument("--disable-extensions")
    chrome_options.add_argument("--headless")
    chrome_options.add_experimental_option('excludeSwitches', ['enable-logging'])

    driver = webdriver.Chrome(service=PATH, options=chrome_options)
    
    return driver
  
  total_pages = 2
  for keyword in keywords:
    for page in tqdm(range(1, total_pages)):
        # Untuk mengambil url dengan keyword yang telah ditetapkan
        # time.sleep(0.3)
        def get_url(keyword):
            keyword = keyword.replace(r' ','+')
            template = 'https://glints.com/id/opportunities/jobs/explore?keyword={}&country=ID&locationName=Indonesia&sortBy=LATEST&page={}'
            url = template.format(keyword,page)
            return url
        
        driver = set_driver()
        driver.implicitly_wait(10)
        #   driver.get(url)
        url = get_url(keyword)
        headers = {'user-agent':'Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/102.0.0.0 Safari/537.36'}

        
        res = requests.get(url, headers=headers) # melakukan request pada link card item untuk masuk ke dalam link tersebut 
        if res.status_code == requests.codes.ok :
            soup = BeautifulSoup(res.content,'lxml') # Fungsi beautifulsoup adalah untuk menerjemah/parsing html pada web yang dituju
        
        soupHTML = str(soup)
        # Untuk mengambil tag card item sesuai jumlah pada masing-masing halaman
        jobs = soup.find_all('div', class_=re.compile(r'.*JobcardContainer.*'))

        # Untuk mengambil link pada masing-masing card item sesuai jumlah pada masing-masing halaman
        def get_link():
            re_href = r'<a class="CompactOpportunityCardsc__CardAnchorWrapper-sc-1y4v110-18 iOjUdU job-search-results_job-card_link" href="(.*?)" target="_blank">'
            href = re.findall(re_href, soupHTML)
            
            return href

        links = []
        for link in get_link():
            link = f'https://glints.com{link}'
            links.append(link)
            
        #  Elemen tag yang diambil dan yang akan dijadikan sebagai data
        def get_data(job):
            link = links[job]
            driver.get(link)
            link_soup = BeautifulSoup(driver.page_source, 'lxml') # melakukan parsing html pada link card item tersebut
            container = link_soup.find('main',class_=re.compile('Opportunitysc__Main.*')) # mengambil tag parent yang merupakan container dari isi informasi seperti nama pekerjaan, perusahaan dan lain-lain

            containerHTML = str(container)            
            # mengambil tag nama pekerjaan
            if re.findall(r'href="/id/companies/', containerHTML):
                re_job_name = r'<h1 class="TopFoldsc__JobOverViewTitle-sc-kklg8i-3 fFAcsE">(.*?)</h1>'
                job_name = re.findall(re_job_name, containerHTML)
            else:
                job_name = 'None'
            
            # mengambil tag nama perusahaan
            if re.findall(r'JobOverViewCompanyName', containerHTML):
                re_company_name = r'<div class="TopFoldsc__JobOverViewCompanyName-sc-kklg8i-5 eLQvRY"><a href=".*?">(.*?)</a></div>'
                company_name = re.findall(re_company_name, containerHTML)
            else:
                re_company_name = r'<span class="TopFoldsc__CompanyName-sc-kklg8i-29 hgrOYX">(.*?)</span>'
                company_name = re.findall(re_company_name, containerHTML)


            # # mengambil tag lokasi perusahaan
            if re.findall(r'JobOverViewCompanyLocation', containerHTML):
                locationHTML = container.find('div', class_=re.compile('.*CompanyLocation.*'))
                locationHTML = str(locationHTML)
                re_location = r'<a href="/id/location/.*">(.*?)</a>'
                location = re.findall(re_location, locationHTML)
            else :
                location = 'None'

            # # mengambil tag persyaratan pekerjaan
            requirement_tag = container.find('div', class_=re.compile('.*JobDescriptionContainer.*'))
            if requirement_tag is not None:
                for requirement in requirement_tag.find_all('div', class_=re.compile('.*DescriptionContainer.*')):
                    requirement = requirement.text
            else:
                requirement = 'None'
            
            # # mengambil tag waktu dari pekerjaan itu di posting
            if re.findall(r'JobOverviewTime', containerHTML):
                re_posted = r'<span class="TopFoldsc__UpdatedAt-sc-kklg8i-12 bYndtI" data-recent="true">(.*?)</span>'
                posted = re.findall(re_posted, containerHTML)
            else:
                re_posted = r'<span data-recent="false" class="CompactOpportunityCardsc__UpdatedAtMessage-sc-1y4v110-17 jgBEKn">(.*?)</span>'
                posted = re.findall(re_posted, soupHTML)
            
            # # mengambil tag gambar perusahaan   
            if re.findall(r'CompanyLogo', containerHTML):
                imageHTML = container.find('div', class_=re.compile('CompanyLogo.*'))
                imageHTML = str(imageHTML)
                re_image = r'src="(.*?)"'
                image = re.findall(re_image, imageHTML)
            else:
                image = 'None'
                        
            data = [job_name,company_name,location,requirement,posted,image,link]
            return data

        records=[]
    
        if len(jobs) != 0:
            for job in range(len(jobs)):
                try:
                    record = get_data(job)
                    records.append(record)
                except ConnectionError:
                    print("Connection Error")
                    break
                except requests.ConnectTimeout:
                    print("Connection Timeout")
                    break
                except Exception as e:
                    print("Error "+ str(e))
                except:
                    pass
        else:
            break

        df = pd.DataFrame(records, columns=['title','company','location','requirement','posted','image','link'])
        temp = pd.concat([temp,df])


    temp.reset_index(inplace=True, drop=True)
    data_df = temp[['title','company','location','requirement','posted','image','link']].copy()

    data_df = data_df.dropna(how='any')
    data_df = data_df.reset_index(drop=True)
    data_df['title'] = data_df['title'].str.get(0)
    data_df['company'] = data_df['company'].str.get(0)
    data_df['location'] = data_df['location'].str.get(0)
    data_df['posted'] = data_df['posted'].str.get(0)
    data_df['image'] = data_df['image'].str.get(0)

    datetime_posted = []
    for i in data_df['posted']:
        if re.findall(r'[0-9]+ (hari|day)', i):
            toSingleString = ''.join(re.findall(r'[0-9]+',i))           
            toInt = (datetime.datetime.today() - datetime.timedelta(int(toSingleString))).strftime('%Y-%m-%d')
            datetime_posted.append(toInt)       
        elif re.findall(r'[0-9]+ (bulan|month)', i):
            toSingleString = ''.join(re.findall(r'[0-9]+',i))
            toInt = (datetime.datetime.today() - relativedelta(months=int(toSingleString))).strftime('%Y-%m-%d')
            datetime_posted.append(toInt)
        else:
            datetime_posted.append(datetime.datetime.today().strftime('%Y-%m-%d'))
    
    data_df['datetime_posted'] = datetime_posted

    data_df_for_csv = data_df.copy()

    data_df_for_csv = data_df_for_csv.replace(r'\n','', regex=True)
    data_df_for_csv = data_df_for_csv.replace(r'\t','', regex=True)
    data_df_for_csv = data_df_for_csv.replace(r'\r','', regex=True)
    data_df_for_csv = data_df_for_csv.replace(r'\s\s\s','', regex=True)
    data_df_for_csv = data_df_for_csv.replace(r'Bisa kerja remote','', regex=True)
    data_df_for_csv['location'] = [re.sub(r'(\w)([A-Z])', r'\1, \2', ele) for ele in data_df_for_csv['location']]
    data_df_for_csv['requirement'] = [re.sub(r'([a-zA-Z])([A-Z][a-z]+)', r'\1. \2', ele) for ele in data_df_for_csv['requirement']]
   
    
  datetime_scrape = datetime.datetime.today().strftime("%Y-%m-%d")
  data_df_for_csv.to_excel(f'data_csv\jobs_data_glints({datetime_scrape} {len(data_df)}).xlsx',index=False)
  
  # db_mysql.insertData(data_df, tableName)
  # db_mysql.removeDuplicate(tableName)

  net_stat = psutil.net_io_counters()
  net_in_end = net_stat.bytes_recv
  net_out_end = net_stat.bytes_sent
  end = time.time()

  print('Scrape is finished..')
  print('Waktu scraping data glints: ', end-start)
  print('Data usage :', psutil.net_io_counters())
  print('Data usage IN:', net_in_end-net_in_start, ', Data usage OUT:', net_out_end-net_out_start) 
  print('total', len(data_df.index))
  
  return data_df

scrapeData('public.scrape_items')